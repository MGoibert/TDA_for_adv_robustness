{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imp import reload\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-22 20:02:30,607 - Devices - INFO - Found 0 devices compatible with CUDA\n",
      "2020-05-22 20:02:31,890 - Cache - INFO - Cache root /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'embedding_type': 'PersistentDiagram'}]\n"
     ]
    }
   ],
   "source": [
    "### Read-in experiment config\n",
    "from collections import namedtuple\n",
    "from tda.experiments.ocsvm_detector.main_mnist_lenet import experiment_plan\n",
    "from tda.embeddings import ThresholdStrategy\n",
    "\n",
    "config = experiment_plan.experiments[0].config\n",
    "\n",
    "# fix some quirks\n",
    "config[\"n_jobs\"] = 4  # increase, if running on a big machine\n",
    "config[\"all_eps\"] = [.01, .02, .05, .1, .2, .3, .4]\n",
    "\n",
    "if \"mnist_lenet\" in config[\"architecture\"]:\n",
    "    config[\"epochs\"] = 50\n",
    "    config[\"threshold_strategy\"] = ThresholdStrategy.ActivationValue\n",
    "    config[\"thresholds\"] = \"_\".join([\"0.1\"] * 4)\n",
    "elif \"svhn\" in config[\"architecture\"]:\n",
    "    config[\"epochs\"] = 300\n",
    "else:\n",
    "    raise NotImplementedError(config[\"architecture\"])\n",
    "\n",
    "# convert config to namedtuple\n",
    "Config = namedtuple(\"Config\", sorted(config))\n",
    "config = Config(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-22 20:02:40,198 - Datasets - INFO - Instantiated dataset MNIST with validation_size 1000\n",
      "/home/elvis/anaconda3/envs/art/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'tda.models.architectures.architecture.Architecture' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/elvis/anaconda3/envs/art/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/elvis/anaconda3/envs/art/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "2020-05-22 20:02:40,251 - Models - INFO - Loaded successfully model from /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../trained_models/mnist_lenet_e_50.model\n"
     ]
    }
   ],
   "source": [
    "# Build dataset and model architecture\n",
    "from tda.models.architectures import get_architecture\n",
    "from tda.models import get_deep_model, Dataset\n",
    "\n",
    "architecture = get_architecture(config.architecture)\n",
    "dataset = Dataset.get_or_create(name=config.dataset)\n",
    "\n",
    "architecture = get_deep_model(\n",
    "    num_epochs=config.epochs,\n",
    "    dataset=dataset,\n",
    "    architecture=architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5760, 784)\n",
      "(1440, 5760)\n",
      "(1280, 1440)\n",
      "(320, 1280)\n",
      "(50, 320)\n",
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "# Check that everything looks okay\n",
    "from tda.graph import Graph\n",
    "\n",
    "x = dataset.train_dataset[0][0]\n",
    "graph = Graph.from_architecture_and_data_point(architecture, x)\n",
    "for key in graph._edge_dict:\n",
    "    layer_matrix = graph._edge_dict[key]\n",
    "    print(layer_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-22 20:02:41,040 - Thresholds - INFO - Detected legacy format for thresholds\n",
      "2020-05-22 20:02:41,041 - Thresholds - INFO - My received thresholds {(-1, 0): 0.1, (0, 1): 0.1, (1, 2): 0.1, (2, 3): 0.1}\n",
      "2020-05-22 20:02:41,048 - GraphDataset - INFO - Using source dataset mnist\n",
      "2020-05-22 20:02:41,049 - GraphDataset - INFO - Checking that the received architecture has been trained\n",
      "2020-05-22 20:02:41,049 - GraphDataset - INFO - OK ! Architecture is ready\n",
      "2020-05-22 20:02:41,049 - GraphDataset - INFO - I am going to generate a dataset of 10 points...\n",
      "2020-05-22 20:02:41,050 - GraphDataset - INFO - Only successful adversaries ? no\n",
      "2020-05-22 20:02:41,050 - GraphDataset - INFO - Which attack ? FGSM\n",
      "2020-05-22 20:02:41,475 - GraphDataset - INFO - computing sample number = 10/10\n",
      "2020-05-22 20:02:41,531 - Cache - INFO - Creating cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=False_archi=mnist_lenet_e_50_compute_graph=True_dataset=mnist_dataset_size=10_epsilon=0.0_noise=0.0_succ_adv=False_train=False.cached for the call to get_sample_dataset\n",
      "2020-05-22 20:02:41,573 - GraphStats - INFO - The data point: y = 3, y_pred = 3 and adv = False\n",
      "2020-05-22 20:02:41,618 - GraphStats - INFO - The data point: y = 8, y_pred = 8 and adv = False\n",
      "2020-05-22 20:02:41,730 - GraphStats - INFO - The data point: y = 9, y_pred = 9 and adv = False\n",
      "2020-05-22 20:02:41,836 - GraphStats - INFO - The data point: y = 1, y_pred = 1 and adv = False\n",
      "2020-05-22 20:02:41,989 - GraphStats - INFO - The data point: y = 6, y_pred = 6 and adv = False\n",
      "2020-05-22 20:02:42,162 - GraphStats - INFO - The data point: y = 8, y_pred = 8 and adv = False\n",
      "2020-05-22 20:02:42,329 - GraphStats - INFO - The data point: y = 6, y_pred = 6 and adv = False\n",
      "2020-05-22 20:02:42,509 - GraphStats - INFO - The data point: y = 8, y_pred = 8 and adv = False\n",
      "2020-05-22 20:02:42,715 - GraphStats - INFO - The data point: y = 6, y_pred = 6 and adv = False\n",
      "2020-05-22 20:02:43,004 - GraphStats - INFO - The data point: y = 3, y_pred = 3 and adv = False\n",
      "2020-05-22 20:02:43,351 - GraphStats - INFO - Number of edges > 0 in link (-1, 0): 395300\n",
      "2020-05-22 20:02:46,667 - GraphStats - INFO - Number of edges > 0 in link (0, 1): 7303\n",
      "2020-05-22 20:02:46,812 - GraphStats - INFO - Number of edges > 0 in link (1, 2): 1745880\n",
      "2020-05-22 20:03:01,707 - GraphStats - INFO - Number of edges > 0 in link (2, 3): 1063\n",
      "2020-05-22 20:03:01,721 - GraphStats - INFO - Number of edges > 0 in link (3, 4): 53150\n",
      "2020-05-22 20:03:02,253 - GraphStats - INFO - Number of edges > 0 in link (5, 6): 1780\n",
      "2020-05-22 20:03:02,281 - Cache - INFO - Creating cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_stats/architecture=mnist_lenet_e_50_dataset=mnist_dataset_size=10.cached for the call to get_stats\n",
      "2020-05-22 20:03:02,296 - Thresholds - INFO - Link (-1, 0): threshold=17354.219336489055 (quantile 0.1)\n",
      "2020-05-22 20:03:02,296 - Thresholds - INFO - Link (0, 1): threshold=63804.91006183071 (quantile 0.1)\n",
      "2020-05-22 20:03:02,305 - Thresholds - INFO - Link (1, 2): threshold=4958.864435989321 (quantile 0.1)\n",
      "2020-05-22 20:03:02,306 - Thresholds - INFO - Link (2, 3): threshold=591506.3446615 (quantile 0.1)\n",
      "2020-05-22 20:03:02,306 - Thresholds - INFO - Thresholds = {(-1, 0): 17354.219336489055, (0, 1): 63804.91006183071, (1, 2): 4958.864435989321, (2, 3): 591506.3446615}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge thresholds for activation graphs:\n",
      "  0 --> 1: 17354.22\n",
      "  1 --> 2: 63804.91\n",
      "  2 --> 3: 4958.86\n",
      "  3 --> 4: 591506.34\n"
     ]
    }
   ],
   "source": [
    "# Compute thresholds\n",
    "from tda.threshold_underoptimized_edges import process_thresholds_underopt\n",
    "from tda.thresholds import process_thresholds\n",
    "\n",
    "thresholds = None\n",
    "edges_to_keep = None\n",
    "raw_thresholds = config.thresholds\n",
    "if config.threshold_strategy == ThresholdStrategy.ActivationValue:\n",
    "    if \":\" in raw_thresholds:\n",
    "        raw_thresholds = \"_\".join(raw_thresholds.split(\":\"))\n",
    "    thresholds = process_thresholds(architecture=architecture,\n",
    "                                    dataset=dataset,\n",
    "                                    raw_thresholds=raw_thresholds,\n",
    "                                    dataset_size=10  # for reduced mem\n",
    "                                    )\n",
    "elif config.threshold_strategy == ThresholdStrategy.UnderoptimizedMagnitudeIncrease:\n",
    "    edges_to_keep = process_thresholds_underopt(\n",
    "        architecture=architecture,\n",
    "        raw_thresholds=raw_thresholds,\n",
    "        method=config.threshold_strategy,\n",
    "        thresholds_are_low_pass=True)\n",
    "else:\n",
    "    raise NotImplementedError(config.threshold_strategy)\n",
    "if thresholds is not None:\n",
    "    print(\"Edge thresholds for activation graphs:\")\n",
    "    for (src, dst), val in thresholds.items():\n",
    "        print(\"  %s --> %s: %.2f\" % (src + 1, dst + 1, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-22 20:03:02,616 - C3PO - INFO - I will produce for you the protocolar datasets !\n",
      "2020-05-22 20:03:02,616 - GraphDataset - INFO - Using source dataset mnist\n",
      "2020-05-22 20:03:02,617 - GraphDataset - INFO - Checking that the received architecture has been trained\n",
      "2020-05-22 20:03:02,617 - GraphDataset - INFO - OK ! Architecture is ready\n",
      "2020-05-22 20:03:02,617 - GraphDataset - INFO - I am going to generate a dataset of 50 points...\n",
      "2020-05-22 20:03:02,618 - GraphDataset - INFO - Only successful adversaries ? yes\n",
      "2020-05-22 20:03:02,618 - GraphDataset - INFO - Which attack ? FGSM\n",
      "2020-05-22 20:03:02,626 - GraphDataset - INFO - computing sample number = 10/50\n",
      "2020-05-22 20:03:02,633 - GraphDataset - INFO - computing sample number = 20/50\n",
      "2020-05-22 20:03:02,640 - GraphDataset - INFO - computing sample number = 30/50\n",
      "2020-05-22 20:03:02,646 - GraphDataset - INFO - computing sample number = 40/50\n",
      "2020-05-22 20:03:02,653 - GraphDataset - INFO - computing sample number = 50/50\n",
      "2020-05-22 20:03:02,654 - Cache - INFO - Creating cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=False_archi=mnist_lenet_e_50_attack_type=FGSM_compute_graph=False_dataset=mnist_dataset_size=50_epsilon=0.0_lims=(0.0, 1.0)_noise=0.0_offset=0_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n",
      "2020-05-22 20:03:02,656 - GraphDataset - INFO - Using source dataset mnist\n",
      "2020-05-22 20:03:02,657 - GraphDataset - INFO - Checking that the received architecture has been trained\n",
      "2020-05-22 20:03:02,657 - GraphDataset - INFO - OK ! Architecture is ready\n",
      "2020-05-22 20:03:02,657 - GraphDataset - INFO - I am going to generate a dataset of 50 points...\n",
      "2020-05-22 20:03:02,658 - GraphDataset - INFO - Only successful adversaries ? yes\n",
      "2020-05-22 20:03:02,658 - GraphDataset - INFO - Which attack ? FGSM\n",
      "2020-05-22 20:03:02,665 - GraphDataset - INFO - computing sample number = 10/50\n",
      "2020-05-22 20:03:02,672 - GraphDataset - INFO - computing sample number = 20/50\n",
      "2020-05-22 20:03:02,679 - GraphDataset - INFO - computing sample number = 30/50\n",
      "2020-05-22 20:03:02,686 - GraphDataset - INFO - computing sample number = 40/50\n",
      "2020-05-22 20:03:02,692 - GraphDataset - INFO - computing sample number = 50/50\n",
      "2020-05-22 20:03:02,693 - Cache - INFO - Creating cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=False_archi=mnist_lenet_e_50_attack_type=FGSM_compute_graph=False_dataset=mnist_dataset_size=50_epsilon=0.0_lims=(0.0, 1.0)_noise=0.0_offset=50_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n",
      "2020-05-22 20:03:02,696 - GraphDataset - INFO - Using source dataset mnist\n",
      "2020-05-22 20:03:02,696 - GraphDataset - INFO - Checking that the received architecture has been trained\n",
      "2020-05-22 20:03:02,697 - GraphDataset - INFO - OK ! Architecture is ready\n",
      "2020-05-22 20:03:02,697 - GraphDataset - INFO - I am going to generate a dataset of 100 points...\n",
      "2020-05-22 20:03:02,697 - GraphDataset - INFO - Only successful adversaries ? yes\n",
      "2020-05-22 20:03:02,698 - GraphDataset - INFO - Which attack ? FGSM\n",
      "2020-05-22 20:03:05,465 - GraphDataset - INFO - computing sample number = 10/100\n",
      "2020-05-22 20:03:07,623 - GraphDataset - INFO - computing sample number = 20/100\n",
      "2020-05-22 20:03:09,495 - GraphDataset - INFO - computing sample number = 30/100\n",
      "2020-05-22 20:03:12,558 - GraphDataset - INFO - computing sample number = 40/100\n",
      "2020-05-22 20:03:14,915 - GraphDataset - INFO - computing sample number = 50/100\n",
      "2020-05-22 20:03:17,374 - GraphDataset - INFO - computing sample number = 60/100\n",
      "2020-05-22 20:03:20,961 - GraphDataset - INFO - computing sample number = 70/100\n",
      "2020-05-22 20:03:22,782 - GraphDataset - INFO - computing sample number = 80/100\n",
      "2020-05-22 20:03:27,153 - GraphDataset - INFO - computing sample number = 90/100\n",
      "2020-05-22 20:03:29,896 - GraphDataset - INFO - computing sample number = 100/100\n",
      "2020-05-22 20:03:29,897 - Cache - INFO - Creating cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=True_archi=mnist_lenet_e_50_attack_type=FGSM_compute_graph=False_dataset=mnist_dataset_size=100_epsilon=0.01_lims=(0.0, 1.0)_noise=0.0_num_iter=100_offset=100_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n",
      "2020-05-22 20:03:29,905 - GraphDataset - INFO - Using source dataset mnist\n",
      "2020-05-22 20:03:29,906 - GraphDataset - INFO - Checking that the received architecture has been trained\n",
      "2020-05-22 20:03:29,908 - GraphDataset - INFO - OK ! Architecture is ready\n",
      "2020-05-22 20:03:29,910 - GraphDataset - INFO - I am going to generate a dataset of 100 points...\n",
      "2020-05-22 20:03:29,911 - GraphDataset - INFO - Only successful adversaries ? yes\n",
      "2020-05-22 20:03:29,912 - GraphDataset - INFO - Which attack ? FGSM\n",
      "2020-05-22 20:03:31,076 - GraphDataset - INFO - computing sample number = 10/100\n",
      "2020-05-22 20:03:32,520 - GraphDataset - INFO - computing sample number = 20/100\n",
      "2020-05-22 20:03:33,552 - GraphDataset - INFO - computing sample number = 30/100\n",
      "2020-05-22 20:03:34,437 - GraphDataset - INFO - computing sample number = 40/100\n",
      "2020-05-22 20:03:35,755 - GraphDataset - INFO - computing sample number = 50/100\n",
      "2020-05-22 20:03:37,599 - GraphDataset - INFO - computing sample number = 60/100\n",
      "2020-05-22 20:03:38,373 - GraphDataset - INFO - computing sample number = 70/100\n",
      "2020-05-22 20:03:39,823 - GraphDataset - INFO - computing sample number = 80/100\n",
      "2020-05-22 20:03:40,475 - GraphDataset - INFO - computing sample number = 90/100\n",
      "2020-05-22 20:03:41,282 - GraphDataset - INFO - computing sample number = 100/100\n",
      "2020-05-22 20:03:41,282 - Cache - INFO - Creating cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=True_archi=mnist_lenet_e_50_attack_type=FGSM_compute_graph=False_dataset=mnist_dataset_size=100_epsilon=0.02_lims=(0.0, 1.0)_noise=0.0_num_iter=100_offset=100_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n",
      "2020-05-22 20:03:41,293 - GraphDataset - INFO - Using source dataset mnist\n",
      "2020-05-22 20:03:41,293 - GraphDataset - INFO - Checking that the received architecture has been trained\n",
      "2020-05-22 20:03:41,299 - GraphDataset - INFO - OK ! Architecture is ready\n",
      "2020-05-22 20:03:41,300 - GraphDataset - INFO - I am going to generate a dataset of 100 points...\n",
      "2020-05-22 20:03:41,301 - GraphDataset - INFO - Only successful adversaries ? yes\n",
      "2020-05-22 20:03:41,302 - GraphDataset - INFO - Which attack ? FGSM\n",
      "2020-05-22 20:03:41,678 - GraphDataset - INFO - computing sample number = 10/100\n",
      "2020-05-22 20:03:41,873 - GraphDataset - INFO - computing sample number = 20/100\n",
      "2020-05-22 20:03:42,285 - GraphDataset - INFO - computing sample number = 30/100\n",
      "2020-05-22 20:03:43,269 - GraphDataset - INFO - computing sample number = 40/100\n",
      "2020-05-22 20:03:43,871 - GraphDataset - INFO - computing sample number = 50/100\n",
      "2020-05-22 20:03:44,043 - GraphDataset - INFO - computing sample number = 60/100\n",
      "2020-05-22 20:03:44,466 - GraphDataset - INFO - computing sample number = 70/100\n",
      "2020-05-22 20:03:44,713 - GraphDataset - INFO - computing sample number = 80/100\n",
      "2020-05-22 20:03:44,868 - GraphDataset - INFO - computing sample number = 90/100\n",
      "2020-05-22 20:03:45,204 - GraphDataset - INFO - computing sample number = 100/100\n",
      "2020-05-22 20:03:45,205 - Cache - INFO - Creating cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=True_archi=mnist_lenet_e_50_attack_type=FGSM_compute_graph=False_dataset=mnist_dataset_size=100_epsilon=0.05_lims=(0.0, 1.0)_noise=0.0_num_iter=100_offset=100_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n",
      "2020-05-22 20:03:45,213 - GraphDataset - INFO - Using source dataset mnist\n",
      "2020-05-22 20:03:45,214 - GraphDataset - INFO - Checking that the received architecture has been trained\n",
      "2020-05-22 20:03:45,216 - GraphDataset - INFO - OK ! Architecture is ready\n",
      "2020-05-22 20:03:45,218 - GraphDataset - INFO - I am going to generate a dataset of 100 points...\n",
      "2020-05-22 20:03:45,218 - GraphDataset - INFO - Only successful adversaries ? yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-22 20:03:45,219 - GraphDataset - INFO - Which attack ? FGSM\n",
      "2020-05-22 20:03:45,316 - GraphDataset - INFO - computing sample number = 10/100\n",
      "2020-05-22 20:03:45,492 - GraphDataset - INFO - computing sample number = 20/100\n",
      "2020-05-22 20:03:45,666 - GraphDataset - INFO - computing sample number = 30/100\n",
      "2020-05-22 20:03:45,755 - GraphDataset - INFO - computing sample number = 40/100\n",
      "2020-05-22 20:03:45,829 - GraphDataset - INFO - computing sample number = 50/100\n",
      "2020-05-22 20:03:45,937 - GraphDataset - INFO - computing sample number = 60/100\n",
      "2020-05-22 20:03:46,050 - GraphDataset - INFO - computing sample number = 70/100\n",
      "2020-05-22 20:03:46,259 - GraphDataset - INFO - computing sample number = 80/100\n",
      "2020-05-22 20:03:46,419 - GraphDataset - INFO - computing sample number = 90/100\n",
      "2020-05-22 20:03:46,526 - GraphDataset - INFO - computing sample number = 100/100\n",
      "2020-05-22 20:03:46,527 - Cache - INFO - Creating cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=True_archi=mnist_lenet_e_50_attack_type=FGSM_compute_graph=False_dataset=mnist_dataset_size=100_epsilon=0.1_lims=(0.0, 1.0)_noise=0.0_num_iter=100_offset=100_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n",
      "2020-05-22 20:03:46,535 - GraphDataset - INFO - Using source dataset mnist\n",
      "2020-05-22 20:03:46,535 - GraphDataset - INFO - Checking that the received architecture has been trained\n",
      "2020-05-22 20:03:46,536 - GraphDataset - INFO - OK ! Architecture is ready\n",
      "2020-05-22 20:03:46,538 - GraphDataset - INFO - I am going to generate a dataset of 100 points...\n",
      "2020-05-22 20:03:46,539 - GraphDataset - INFO - Only successful adversaries ? yes\n",
      "2020-05-22 20:03:46,540 - GraphDataset - INFO - Which attack ? FGSM\n",
      "2020-05-22 20:03:46,593 - GraphDataset - INFO - computing sample number = 10/100\n",
      "2020-05-22 20:03:46,621 - GraphDataset - INFO - computing sample number = 20/100\n",
      "2020-05-22 20:03:46,677 - GraphDataset - INFO - computing sample number = 30/100\n",
      "2020-05-22 20:03:46,743 - GraphDataset - INFO - computing sample number = 40/100\n",
      "2020-05-22 20:03:46,781 - GraphDataset - INFO - computing sample number = 50/100\n",
      "2020-05-22 20:03:46,827 - GraphDataset - INFO - computing sample number = 60/100\n",
      "2020-05-22 20:03:46,872 - GraphDataset - INFO - computing sample number = 70/100\n",
      "2020-05-22 20:03:46,935 - GraphDataset - INFO - computing sample number = 80/100\n",
      "2020-05-22 20:03:46,992 - GraphDataset - INFO - computing sample number = 90/100\n",
      "2020-05-22 20:03:47,031 - GraphDataset - INFO - computing sample number = 100/100\n",
      "2020-05-22 20:03:47,031 - Cache - INFO - Creating cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=True_archi=mnist_lenet_e_50_attack_type=FGSM_compute_graph=False_dataset=mnist_dataset_size=100_epsilon=0.2_lims=(0.0, 1.0)_noise=0.0_num_iter=100_offset=100_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n",
      "2020-05-22 20:03:47,039 - GraphDataset - INFO - Using source dataset mnist\n",
      "2020-05-22 20:03:47,040 - GraphDataset - INFO - Checking that the received architecture has been trained\n",
      "2020-05-22 20:03:47,041 - GraphDataset - INFO - OK ! Architecture is ready\n",
      "2020-05-22 20:03:47,043 - GraphDataset - INFO - I am going to generate a dataset of 100 points...\n",
      "2020-05-22 20:03:47,044 - GraphDataset - INFO - Only successful adversaries ? yes\n",
      "2020-05-22 20:03:47,046 - GraphDataset - INFO - Which attack ? FGSM\n",
      "2020-05-22 20:03:47,075 - GraphDataset - INFO - computing sample number = 10/100\n",
      "2020-05-22 20:03:47,102 - GraphDataset - INFO - computing sample number = 20/100\n",
      "2020-05-22 20:03:47,128 - GraphDataset - INFO - computing sample number = 30/100\n",
      "2020-05-22 20:03:47,183 - GraphDataset - INFO - computing sample number = 40/100\n",
      "2020-05-22 20:03:47,218 - GraphDataset - INFO - computing sample number = 50/100\n",
      "2020-05-22 20:03:47,245 - GraphDataset - INFO - computing sample number = 60/100\n",
      "2020-05-22 20:03:47,270 - GraphDataset - INFO - computing sample number = 70/100\n",
      "2020-05-22 20:03:47,302 - GraphDataset - INFO - computing sample number = 80/100\n",
      "2020-05-22 20:03:47,329 - GraphDataset - INFO - computing sample number = 90/100\n",
      "2020-05-22 20:03:47,360 - GraphDataset - INFO - computing sample number = 100/100\n",
      "2020-05-22 20:03:47,363 - Cache - INFO - Creating cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=True_archi=mnist_lenet_e_50_attack_type=FGSM_compute_graph=False_dataset=mnist_dataset_size=100_epsilon=0.3_lims=(0.0, 1.0)_noise=0.0_num_iter=100_offset=100_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n",
      "2020-05-22 20:03:47,371 - GraphDataset - INFO - Using source dataset mnist\n",
      "2020-05-22 20:03:47,371 - GraphDataset - INFO - Checking that the received architecture has been trained\n",
      "2020-05-22 20:03:47,386 - GraphDataset - INFO - OK ! Architecture is ready\n",
      "2020-05-22 20:03:47,387 - GraphDataset - INFO - I am going to generate a dataset of 100 points...\n",
      "2020-05-22 20:03:47,390 - GraphDataset - INFO - Only successful adversaries ? yes\n",
      "2020-05-22 20:03:47,391 - GraphDataset - INFO - Which attack ? FGSM\n",
      "2020-05-22 20:03:47,414 - GraphDataset - INFO - computing sample number = 10/100\n",
      "2020-05-22 20:03:47,438 - GraphDataset - INFO - computing sample number = 20/100\n",
      "2020-05-22 20:03:47,463 - GraphDataset - INFO - computing sample number = 30/100\n",
      "2020-05-22 20:03:47,489 - GraphDataset - INFO - computing sample number = 40/100\n",
      "2020-05-22 20:03:47,520 - GraphDataset - INFO - computing sample number = 50/100\n",
      "2020-05-22 20:03:47,546 - GraphDataset - INFO - computing sample number = 60/100\n",
      "2020-05-22 20:03:47,569 - GraphDataset - INFO - computing sample number = 70/100\n",
      "2020-05-22 20:03:47,594 - GraphDataset - INFO - computing sample number = 80/100\n",
      "2020-05-22 20:03:47,622 - GraphDataset - INFO - computing sample number = 90/100\n",
      "2020-05-22 20:03:47,684 - GraphDataset - INFO - computing sample number = 100/100\n",
      "2020-05-22 20:03:47,685 - Cache - INFO - Creating cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=True_archi=mnist_lenet_e_50_attack_type=FGSM_compute_graph=False_dataset=mnist_dataset_size=100_epsilon=0.4_lims=(0.0, 1.0)_noise=0.0_num_iter=100_offset=100_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n"
     ]
    }
   ],
   "source": [
    "# Build tda dataset (i.e activation graphs for clean and adversarial inputs)\n",
    "from tda.protocol import get_protocolar_datasets\n",
    "\n",
    "# %debug\n",
    "import torch\n",
    "X_train, _ = zip(*dataset.train_dataset)\n",
    "X_train = torch.stack(X_train)\n",
    "lims = X_train.min().item(), X_train.max().item()\n",
    "all_epsilons = [0.01, .02, 0.05, 0.1, 0.2, 0.3, 0.4]\n",
    "(train_clean, test_clean, train_adv,\n",
    " test_adv) = get_protocolar_datasets(dataset=dataset,\n",
    "                                     succ_adv=True,\n",
    "                                     dataset_size=config.dataset_size,\n",
    "                                     noise=0., compute_graph=False,\n",
    "                                     all_epsilons=all_epsilons,\n",
    "                                     attack_type=\"FGSM\",\n",
    "                                     archi=architecture,\n",
    "                                     lims=lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing test adversarial embeddings for eps=0.010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f2b2a68b500c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     embeddings[eps] = Parallel(n_jobs=n_jobs)(\n\u001b[1;32m     23\u001b[0m         \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         for line in test_adv[eps])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/art/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/art/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/art/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/art/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/art/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compute embeddings for test-set adversarial inputs\n",
    "from tda.embeddings import (get_embedding, EmbeddingType,\n",
    "                            KernelType, ThresholdStrategy)\n",
    "from joblib import delayed, Parallel\n",
    "\n",
    "\n",
    "def embedding_getter(line):\n",
    "    embedding = get_embedding(\n",
    "        architecture=architecture,\n",
    "        embedding_type=EmbeddingType.PersistentDiagram,\n",
    "        line=line, dataset=dataset,\n",
    "        threshold_strategy=config.threshold_strategy,\n",
    "        thresholds=thresholds, edges_to_keep=edges_to_keep)\n",
    "    print(\".\", end=\"\")\n",
    "    return embedding\n",
    "\n",
    "embeddings = {}\n",
    "for eps in test_adv:\n",
    "    print(\"\\nComputing test adversarial embeddings for eps=%.3f\" % eps)\n",
    "    embeddings[eps] = Parallel(n_jobs=config.n_jobs)(\n",
    "        delayed(embedding_getter)(line)\n",
    "        for line in test_adv[eps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0] = Parallel(n_jobs=config.n_jobs)(\n",
    "        delayed(embedding_getter)(line)\n",
    "        for line in test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_test_adv = dict((eps, embeddings[eps]) for eps in embeddings if eps != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute other adversarial examples\n",
    "embeddings_train_adv = {}\n",
    "for eps in train_adv:\n",
    "    print(\"\\nComputing train adversarial embeddings for eps=%.3f\" % eps)\n",
    "    embeddings_train_adv[eps] = Parallel(n_jobs=config.n_jobs)(\n",
    "        delayed(embedding_getter)(line) for line in train_adv[eps])\n",
    "print(\"\\nComputing train clean embeddings\")\n",
    "embeddings_train_clean = Parallel(n_jobs=config.n_jobs)(\n",
    "        delayed(embedding_getter)(line)\n",
    "        for line in train_clean)\n",
    "print(\"\\nComputing test clean embeddings\")\n",
    "embeddings_test_clean = Parallel(n_jobs=config.n_jobs)(\n",
    "        delayed(embedding_getter)(line)\n",
    "        for line in test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The real deal: try to detect adversarial examples from normal examples\n",
    "from tda.protocol import evaluate_embeddings\n",
    "\n",
    "param_space = [{\"M\": 20, \"sigma\": sigma} for sigma in np.logspace(-3, 3, 7)]\n",
    "kernel_type = KernelType.SlicedWasserstein\n",
    "evaluation_results = evaluate_embeddings(embeddings_train_clean,\n",
    "                                         embeddings_test_clean,\n",
    "                                         embeddings_train_adv,\n",
    "                                         embeddings_test_adv,\n",
    "                                         kernel_type=kernel_type,\n",
    "                                         param_space=param_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize embeddings\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "cmap = cm.Blues_r\n",
    "\n",
    "colors = {0: \"b\",\n",
    "          0.1: \"c\",\n",
    "          0.2: \"m\",\n",
    "          0.3: \"r\"}\n",
    "_, (ax1, ax3, ax4) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for eps in embeddings:\n",
    "    if not eps in [0., 0.1, 0.3]: continue\n",
    "    color = colors[eps]\n",
    "    for x in embeddings[eps]:\n",
    "        birth, death = np.transpose(x)\n",
    "        age = death - birth\n",
    "        ax1.plot(birth, c=color)\n",
    "        ax1.set_ylabel(\"birth\")\n",
    "        ax1.set_xlabel(\"points\")\n",
    "        # ax2.plot(death, c=color)\n",
    "        # ax2.set_ylabel(\"death\")\n",
    "        # ax2.set_xlabel(\"points\")\n",
    "        ax3.plot(age, c=color)\n",
    "        ax3.set_ylabel(\"age (death - birth)\")\n",
    "        ax3.set_xlabel(\"points\")\n",
    "        ax4.scatter(birth, death, c=color);\n",
    "        ax4.set_xlabel(\"birth\")\n",
    "        ax4.set_ylabel(\"death\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance of the detector\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = []\n",
    "for key in [\"supervised_metrics\", \"unsupervised_metrics\"]:\n",
    "    tmp = evaluation_results[key]\n",
    "    if key == \"unsupervised_metrics\":\n",
    "        sup = False\n",
    "    else:\n",
    "        sup = True\n",
    "    for eps in tmp:\n",
    "        df.append(dict(sup=sup, eps=eps, auc=tmp[eps][\"auc\"][\"value\"],\n",
    "                       method=\"PersistentDiagram\",\n",
    "                       arch=architecture.name))\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data=df, x=\"eps\", y=\"auc\", hue=\"sup\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tda.embeddings.persistence_landscape as pls\n",
    "reload(pls)\n",
    "\n",
    "persimgs = {}\n",
    "for eps in sorted(list(embeddings_train_adv.keys())):\n",
    "    persimgs[eps] = pls.persistence_diagrams_to_images(\n",
    "        embeddings_train_adv[eps][::10], n_jobs=config.n_jobs,\n",
    "        bandwidth=1e-4, backend=\"persim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "eps_list = np.array(list(persimgs.keys()))\n",
    "\n",
    "@interact\n",
    "def plot_persistence_images(eps=(0, .5, 0.05),\n",
    "                            example_index=(0, len(persimgs[eps_list[0]]) - 1)):\n",
    "    _, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    closest_i = np.abs(eps - eps_list).argmin()\n",
    "    eps = eps_list[closest_i]\n",
    "    persimg = persimgs[eps][example_index]\n",
    "    ax.imshow(persimg, cmap=plt.get_cmap(\"plasma\"));\n",
    "    ax.axis(\"off\");\n",
    "    ax.matshow(persimg)\n",
    "    # plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    # plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tda.refactor.eval as _eval\n",
    "reload(_eval)\n",
    "\n",
    "scores = []\n",
    "for supervised in [True, False][:1]:\n",
    "    for kernel_type in [KernelType.PersistenceLandscape,\n",
    "                        KernelType.SlicedWasserstein][1:]:\n",
    "        if kernel_type == KernelType.PersistenceLandscape:\n",
    "            param_space = [{\"backend\": \"persim\", \"resolution\": (k, k),\n",
    "                            \"bandwidth\": bw}\n",
    "                           for k in [5, 10, 20][2:] for bw in [1, 3]]\n",
    "        elif kernel_type == KernelType.SlicedWasserstein:\n",
    "            param_space = [dict(M=20, sigma=sigma)\n",
    "                           for sigma in np.logspace(-3, 3, 7)]\n",
    "        else:\n",
    "            raise NotImplementedError(kernel_type)\n",
    "        df = _eval.evaluate_embeddings(kernel_type,\n",
    "                                       embeddings_train_clean,\n",
    "                                       embeddings_test_clean,\n",
    "                                       embeddings_train_adv,\n",
    "                                       embeddings_test_adv,\n",
    "                                       param_space,\n",
    "                                       supervised=supervised,\n",
    "                                       n_jobs=config.n_jobs,\n",
    "                                       random_state=0)\n",
    "        scores.append(df)\n",
    "scores = pd.concat(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import seaborn as sns\n",
    "\n",
    "df = []\n",
    "for _, line in scores.iterrows():\n",
    "    line = line.to_dict()\n",
    "    low = line.pop(\"auc.lower_bound\")\n",
    "    val = line.pop(\"auc.value\")\n",
    "    high = line.pop(\"auc.upper_bound\")\n",
    "    df += [{\"quantile\": q, \"auc\": val, **line}\n",
    "            for q, val in zip([\"lower\", \"middle\", \"upper\"],\n",
    "                              [low, val, high])]\n",
    "df = pd.DataFrame(df)\n",
    "    \n",
    "@interact\n",
    "def plot_scores(supervised=df.supervised.unique()):\n",
    "    sns.catplot(data=df.loc[df.supervised == supervised], x=\"eps\",\n",
    "                y=\"auc\", hue=\"kernel_type\", kind=\"point\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import bmat as sparse_bmat, coo_matrix\n",
    "\n",
    "\n",
    "def toeplitz_1_ch(kernel, input_size):\n",
    "    # shapes\n",
    "    k_h, k_w = kernel.shape\n",
    "    i_h, i_w = input_size\n",
    "    o_h, o_w = i_h-k_h+1, i_w-k_w+1\n",
    "\n",
    "    # construct 1d conv toeplitz matrices for each row of the kernel\n",
    "    toeplitz = []\n",
    "    for r in range(k_h):\n",
    "        toeplitz.append(linalg.toeplitz(c=(kernel[r,0], *np.zeros(i_w-k_w)),\n",
    "                                        r=(*kernel[r], *np.zeros(i_w-k_w))) ) \n",
    "\n",
    "    # construct toeplitz matrix of toeplitz matrices (just for padding=0)\n",
    "    h_blocks, w_blocks = o_h, i_h\n",
    "    h_block, w_block = toeplitz[0].shape\n",
    "\n",
    "    W_conv = np.zeros((h_blocks, h_block, w_blocks, w_block))\n",
    "\n",
    "    for i, B in enumerate(toeplitz):\n",
    "        for j in range(o_h):\n",
    "            W_conv[j, :, i+j, :] = B\n",
    "\n",
    "    W_conv.shape = (h_blocks*h_block, w_blocks*w_block)\n",
    "\n",
    "    return W_conv\n",
    "\n",
    "\n",
    "def toeplitz_mult_ch(kernel, input_size, strides=1, padding=0):\n",
    "    \"\"\"Compute toeplitz matrix for 2d conv with multiple in and out channels.\n",
    "    Args:\n",
    "        kernel: shape=(n_out, n_in, H_k, W_k)\n",
    "        input_size: (n_in, H_i, W_i)\"\"\"\n",
    "\n",
    "    kernel_size = kernel.shape\n",
    "    output_size = (kernel_size[0], input_size[1] - (kernel_size[1] - 1),\n",
    "                   input_size[2] - (kernel_size[2] - 1))\n",
    "    T = []\n",
    "    for i, ks in enumerate(kernel):  # loop over output channel\n",
    "        T_ = []\n",
    "        for j, k in enumerate(ks):  # loop over input channel\n",
    "            Tk = toeplitz_1_ch(k, input_size[1:])\n",
    "            Tk = coo_matrix(Tk)\n",
    "            T_.append(Tk)\n",
    "        T.append(T_)\n",
    "    return sparse_bmat(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (6, 10, 10)\n",
    "kernel_size = (7, input_size[0], 3, 3)\n",
    "k = np.random.randn(np.prod(kernel_size)).reshape(kernel_size)\n",
    "mat = toeplitz_mult_ch(k, input_size)\n",
    "print(mat.shape)\n",
    "# plt.matshow(mat); plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(mat.todense().T);\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(architecture.layers[0].build_matrix().todense().T); plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = architecture.layers[0]\n",
    "\n",
    "for param_ in conv.func.named_parameters():\n",
    "    param = param_[1]\n",
    "    # name_ = param_[0]\n",
    "    if len(param.size()) > 1:\n",
    "        kernel = param.data\n",
    "        break\n",
    "else:\n",
    "    raise RuntimeError\n",
    "kernel = kernel.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (1, 28, 28)\n",
    "mat = toeplitz_mult_ch(kernel, input_size)\n",
    "plt.matshow(mat.todense().T);\n",
    "plt.axis(\"off\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(mat.todense() - architecture.layers[0].build_matrix().todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X_train).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_thresholds.split(\"_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = zip(*dataset.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture.forward(X_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings_train_adv[eps][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(all_epsilons='0.01;0.1', architecture='mnist_lenet', attack_type='FGSM', dataset='MNIST', dataset_size=100, embedding_type='PersistentDiagram', epochs=50, kernel_type='SlicedWasserstein', n_jobs=24, noise=0.0, num_iter=50, sigmoidize=False, threshold_strategy='ActivationValue', thresholds='0.1_0.1_0.1_0.1')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
