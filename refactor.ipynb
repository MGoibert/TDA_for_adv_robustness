{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "root = \"/tmp\"\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "\n",
    "for X_train, y_train in train_loader:\n",
    "    break\n",
    "for X_val, y_val in test_loader:\n",
    "    break\n",
    "    \n",
    "if True:\n",
    "    X_train = X_train.double()\n",
    "    X_val = X_val.double()\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    X_train = X_train.cuda()\n",
    "    y_train = y_train.cuda()\n",
    "    X_test = X_test.cuda()\n",
    "    y_test = y_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "from tda.models import pytorch_lenet\n",
    "lenet = pytorch_lenet.LeNet()\n",
    "lenet.train_or_load()\n",
    "if torch.cuda.is_available():\n",
    "    lenet = lenet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or train model\n",
    "lenet.train_or_load(train_loader=[(X_train, y_train)], val_data=(X_val, y_val),\n",
    "                    num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build architecture (tda pipeline terminology)\n",
    "import tda.models.architectures.parser as parser\n",
    "from imp import reload\n",
    "reload(parser)\n",
    "\n",
    "x = X_train[0]\n",
    "lenet_arch = parser.model_to_architecture(lenet, name=\"mnist_lenet\",\n",
    "                                          x=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tda.graph import Graph\n",
    "\n",
    "graph = Graph.from_architecture_and_data_point(lenet_arch, x)\n",
    "for key in graph._edge_dict:\n",
    "    layer_matrix = graph._edge_dict[key]\n",
    "    print(layer_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute thresholds\n",
    "from tda.thresholds import process_thresholds\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, name, X_train, y_train, X_test, y_test):\n",
    "        self.name = name\n",
    "        self.train_dataset = list(zip(X_train, y_train))\n",
    "        self.test_and_val_dataset = list(zip(X_test, y_test))\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    \n",
    "# %debug\n",
    "threshold = 0.1\n",
    "raw_thresholds = \"_\".join([str(threshold)] * len(lenet_arch.layers))\n",
    "dataset = Dataset(\"mnist\", X_train, y_train, X_val, y_val)\n",
    "thresholds = process_thresholds(architecture=lenet_arch,\n",
    "                                dataset=dataset,\n",
    "                                raw_thresholds=raw_thresholds,\n",
    "                                dataset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tda dataset (i.e activation graphs for clean and adversarial inputs)\n",
    "from tda.protocol import get_protocolar_datasets\n",
    "\n",
    "# %debug\n",
    "lims = X_train.min(), X_train.max()\n",
    "dataset_size = 200\n",
    "all_epsilons = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4]\n",
    "(train_clean, test_clean, train_adv,\n",
    " test_adv) = get_protocolar_datasets(dataset=dataset,\n",
    "                                     succ_adv=True,\n",
    "                                     dataset_size=dataset_size,\n",
    "                                     noise=0.,\n",
    "                                     all_epsilons=all_epsilons,\n",
    "                                     attack_type=\"FGSM_art\",\n",
    "                                     archi=lenet_arch,\n",
    "                                     lims=lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for test-set adversarial inputs\n",
    "from tda.embeddings import get_embedding, EmbeddingType, KernelType, ThresholdStrategy\n",
    "from joblib import delayed, Parallel\n",
    "\n",
    "\n",
    "def embedding_getter(line):\n",
    "    embedding = get_embedding(\n",
    "        architecture=lenet_arch,\n",
    "        embedding_type=EmbeddingType.PersistentDiagram,\n",
    "        line=line, dataset=None, edges_to_keep=None,\n",
    "        threshold_strategy=ThresholdStrategy.ActivationValue,\n",
    "        thresholds=thresholds)\n",
    "    print(\".\", end=\"\")\n",
    "    return embedding\n",
    "\n",
    "\n",
    "n_jobs = 1\n",
    "embeddings = {}\n",
    "for eps in test_adv:\n",
    "    print(\"\\nComputing test adversarial embeddings for eps=%.3f\" % eps)\n",
    "    embeddings[eps] = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(embedding_getter)(line)\n",
    "        for line in test_adv[eps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0] = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(embedding_getter)(line)\n",
    "        for line in test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_test_adv = dict((eps, embeddings[eps]) for eps in embeddings if eps != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute other adversarial examples\n",
    "embeddings_train_adv = {}\n",
    "for eps in train_adv:\n",
    "    print(\"\\nComputing train adversarial embeddings for eps=%.3f\" % eps)\n",
    "    embeddings_train_adv[eps] = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(embedding_getter)(line) for line in train_adv[eps])\n",
    "print(\"\\nComputing train clean embeddings\")\n",
    "embeddings_train_clean = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(embedding_getter)(line)\n",
    "        for line in train_clean)\n",
    "print(\"\\nComputing test clean embeddings\")\n",
    "embeddings_test_clean = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(embedding_getter)(line)\n",
    "        for line in test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The real deal: try to detect adversarial examples from normal examples\n",
    "from tda.protocol import evaluate_embeddings\n",
    "\n",
    "param_space = [{\"M\": 20, \"sigma\": sigma} for sigma in np.logspace(-3, 3, 7)]\n",
    "kernel_type = KernelType.SlicedWasserstein\n",
    "evaluation_results = evaluate_embeddings(embeddings_train_clean,\n",
    "                                         embeddings_test_clean,\n",
    "                                         embeddings_train_adv,\n",
    "                                         embeddings_test_adv,\n",
    "                                         kernel_type=kernel_type,\n",
    "                                         param_space=param_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize embeddings\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "cmap = cm.Blues_r\n",
    "\n",
    "colors = {0: \"b\",\n",
    "          0.1: \"c\",\n",
    "          0.2: \"m\",\n",
    "          0.3: \"r\"}\n",
    "_, (ax1, ax3, ax4) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for eps in embeddings:\n",
    "    if not eps in [0., 0.1, 0.3]: continue\n",
    "    color = colors[eps]\n",
    "    for x in embeddings[eps]:\n",
    "        birth, death = np.transpose(x)\n",
    "        age = death - birth\n",
    "        ax1.plot(birth, c=color)\n",
    "        ax1.set_ylabel(\"birth\")\n",
    "        ax1.set_xlabel(\"points\")\n",
    "        # ax2.plot(death, c=color)\n",
    "        # ax2.set_ylabel(\"death\")\n",
    "        # ax2.set_xlabel(\"points\")\n",
    "        ax3.plot(age, c=color)\n",
    "        ax3.set_ylabel(\"age (death - birth)\")\n",
    "        ax3.set_xlabel(\"points\")\n",
    "        ax4.scatter(birth, death, c=color);\n",
    "        ax4.set_xlabel(\"birth\")\n",
    "        ax4.set_ylabel(\"death\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance of the detector\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = []\n",
    "for key in [\"supervised_metrics\", \"unsupervised_metrics\"]:\n",
    "    tmp = evaluation_results[key]\n",
    "    if key == \"unsupervised_metrics\":\n",
    "        sup = False\n",
    "    else:\n",
    "        sup = True\n",
    "    for eps in tmp:\n",
    "        df.append(dict(sup=sup, eps=eps, auc=tmp[eps][\"auc\"][\"upper_bound\"],\n",
    "                       method=\"PersistentDiagram\",\n",
    "                       arch=lenet_arch.name))\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data=df, x=\"eps\", y=\"auc\", hue=\"sup\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_arch.layers[-2].get_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse.linalg as slinalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V = slinalg.svds(lenet_arch.layers[0].get_matrix()[], k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(15, 8))\n",
    "data = lenet_arch.layers[0].get_matrix()[-1].todense()[:2000].T\n",
    "data = np.ma.masked_where(data == 0, data)\n",
    "ax.matshow(data, cmap=plt.cm.RdBu);\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph._edge_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_arch.layers[4].get_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tda.graph import Graph\n",
    "\n",
    "Graph.from_architecture_and_data_point(lenet_arch, X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph._edge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripser.ripser import Rips\n",
    "\n",
    "rips = Rips(maxdim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rips.fit_transform(graph.get_adjacency_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_list(graph):\n",
    "    \"\"\"\n",
    "    Generate the list of edges of the multipartite graph\n",
    "    \"\"\"\n",
    "    shapes = graph._get_shapes()\n",
    "    all_layer_indices = sorted(list(shapes.keys()))\n",
    "    vertex_offset = [0] + list(np.cumsum([shapes[idx]\n",
    "                                          for idx in all_layer_indices]))\n",
    "    vertex_offset = vertex_offset[:-1]\n",
    "    for source_layer, target_layer in graph._edge_dict:\n",
    "        offset_source = vertex_offset[source_layer + 1]\n",
    "        offset_target = vertex_offset[target_layer + 1]\n",
    "        mat = graph._edge_dict[(source_layer, target_layer)]\n",
    "        source_vertices = mat.col + offset_source\n",
    "        target_vertices = mat.row + offset_target\n",
    "        for edge, weight in zip(zip(source_vertices, target_vertices), mat.data):\n",
    "            yield edge, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit -n 1 graph.get_edge_list();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit -n 1 list(get_edge_list(graph));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.08 / .777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tda.embeddings import persistent_diagrams\n",
    "from tda import graph\n",
    "from imp import reload\n",
    "\n",
    "reload(persistent_diagrams)\n",
    "reload(graph)\n",
    "graph = graph.Graph.from_architecture_and_data_point(lenet_arch, X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toto = persistent_diagrams._prepare_edges_for_diagram_old(graph);\n",
    "titi = persistent_diagrams._prepare_edges_for_diagram_fast(graph);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titi = list(titi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titi[-1], toto[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vertices, w in persistent_diagrams._prepare_edges_for_diagram_fast(graph):\n",
    "    print(vertices, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
