{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "root = \"/tmp\"\n",
    "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "\n",
    "for X_train, y_train in train_loader:\n",
    "    break\n",
    "for X_val, y_val in test_loader:\n",
    "    break\n",
    "    \n",
    "if True:\n",
    "    X_train = X_train.double()\n",
    "    X_val = X_val.double()\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    X_train = X_train.cuda()\n",
    "    y_train = y_train.cuda()\n",
    "    X_test = X_test.cuda()\n",
    "    y_test = y_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LeNet_pretrained.pth\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "from tda.models import pytorch_lenet\n",
    "lenet = pytorch_lenet.LeNet()\n",
    "lenet.train_or_load()\n",
    "if torch.cuda.is_available():\n",
    "    lenet = lenet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LeNet_pretrained.pth\n"
     ]
    }
   ],
   "source": [
    "# Load or train model\n",
    "lenet.train_or_load(train_loader=[(X_train, y_train)], val_data=(X_val, y_val),\n",
    "                    num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-18 14:15:10,938 - ConvLayer - INFO - <tda.models.layers.conv_layer.ConvLayer object at 0x7f7d61da26d0> received input with shape torch.Size([1, 1, 28, 28])\n",
      "2020-05-18 14:15:10,940 - ConvLayer - INFO - <tda.models.layers.conv_layer.ConvLayer object at 0x7f7d61da2790> received input with shape torch.Size([1, 20, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "# Build architecture (tda pipeline terminology)\n",
    "import tda.models.architectures.parser as parser\n",
    "from imp import reload\n",
    "reload(parser)\n",
    "\n",
    "x = X_train[0]\n",
    "lenet_arch = parser.model_to_architecture(lenet, name=\"mnist_lenet\",\n",
    "                                          x=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11520, 784)\n",
      "(2880, 11520)\n",
      "(3200, 2880)\n",
      "(800, 3200)\n",
      "(500, 800)\n",
      "(10, 500)\n"
     ]
    }
   ],
   "source": [
    "from tda.graph import Graph\n",
    "\n",
    "graph = Graph.from_architecture_and_data_point(lenet_arch, x)\n",
    "for key in graph._edge_dict:\n",
    "    layer_matrix = graph._edge_dict[key]\n",
    "    print(layer_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-18 14:15:15,361 - Cache - INFO - Cache root /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache/\n",
      "2020-05-18 14:15:15,373 - Thresholds - INFO - Detected legacy format for thresholds\n",
      "2020-05-18 14:15:15,374 - Thresholds - INFO - My received thresholds {(-1, 0): 0.01, (0, 1): 0.01, (1, 2): 0.01, (2, 3): 0.01, (3, 4): 0.01, (4, 5): 0.01}\n",
      "2020-05-18 14:15:15,375 - Cache - INFO - Using cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_stats/architecture=mnist_lenet_e_0_dataset=mnist_dataset_size=10.cached for the call to get_stats\n",
      "2020-05-18 14:15:15,758 - Thresholds - INFO - Link (-1, 0): threshold=960.4163540137355 (quantile 0.01)\n",
      "2020-05-18 14:15:15,759 - Thresholds - INFO - Link (0, 1): threshold=10469.271519874757 (quantile 0.01)\n",
      "2020-05-18 14:15:15,878 - Thresholds - INFO - Link (1, 2): threshold=37.45387282393914 (quantile 0.01)\n",
      "2020-05-18 14:15:15,879 - Thresholds - INFO - Link (2, 3): threshold=2847.6980840866863 (quantile 0.01)\n",
      "2020-05-18 14:15:15,909 - Thresholds - INFO - Link (3, 4): threshold=12.86001398564172 (quantile 0.01)\n",
      "2020-05-18 14:15:15,910 - Thresholds - INFO - Link (4, 5): threshold=6.085997202467125 (quantile 0.01)\n",
      "2020-05-18 14:15:15,911 - Thresholds - INFO - Thresholds = {(-1, 0): 960.4163540137355, (0, 1): 10469.271519874757, (1, 2): 37.45387282393914, (2, 3): 2847.6980840866863, (3, 4): 12.86001398564172, (4, 5): 6.085997202467125}\n"
     ]
    }
   ],
   "source": [
    "# Compute thresholds\n",
    "from tda.thresholds import process_thresholds\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, name, X_train, y_train, X_test, y_test):\n",
    "        self.name = name\n",
    "        self.train_dataset = list(zip(X_train, y_train))\n",
    "        self.test_and_val_dataset = list(zip(X_test, y_test))\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    \n",
    "# %debug\n",
    "threshold = 0.01\n",
    "raw_thresholds = \"_\".join([str(threshold)] * len(lenet_arch.layers))\n",
    "dataset = Dataset(\"mnist\", X_train, y_train, X_val, y_val)\n",
    "thresholds = process_thresholds(architecture=lenet_arch,\n",
    "                                dataset=dataset,\n",
    "                                raw_thresholds=raw_thresholds,\n",
    "                                dataset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-18 14:15:21,384 - C3PO - INFO - I will produce for you the protocolar datasets !\n",
      "2020-05-18 14:15:21,391 - Cache - INFO - Using cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=False_archi=mnist_lenet_e_0_attack_type=FGSM_art_compute_graph=False_dataset=mnist_dataset_size=100_epsilon=0.0_lims=(tensor(-0.5000), tensor(0.5000))_noise=0.0_offset=0_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n",
      "2020-05-18 14:15:21,409 - Cache - INFO - Using cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=False_archi=mnist_lenet_e_0_attack_type=FGSM_art_compute_graph=False_dataset=mnist_dataset_size=100_epsilon=0.0_lims=(tensor(-0.5000), tensor(0.5000))_noise=0.0_offset=100_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n",
      "2020-05-18 14:15:21,426 - Cache - INFO - Using cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=True_archi=mnist_lenet_e_0_attack_type=FGSM_art_compute_graph=False_dataset=mnist_dataset_size=200_epsilon=0.01_lims=(tensor(-0.5000), tensor(0.5000))_noise=0.0_num_iter=100_offset=200_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n",
      "2020-05-18 14:15:21,437 - Cache - INFO - Using cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=True_archi=mnist_lenet_e_0_attack_type=FGSM_art_compute_graph=False_dataset=mnist_dataset_size=200_epsilon=0.05_lims=(tensor(-0.5000), tensor(0.5000))_noise=0.0_num_iter=100_offset=200_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n",
      "2020-05-18 14:15:21,447 - Cache - INFO - Using cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=True_archi=mnist_lenet_e_0_attack_type=FGSM_art_compute_graph=False_dataset=mnist_dataset_size=200_epsilon=0.1_lims=(tensor(-0.5000), tensor(0.5000))_noise=0.0_num_iter=100_offset=200_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n",
      "2020-05-18 14:15:21,459 - Cache - INFO - Using cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=True_archi=mnist_lenet_e_0_attack_type=FGSM_art_compute_graph=False_dataset=mnist_dataset_size=200_epsilon=0.2_lims=(tensor(-0.5000), tensor(0.5000))_noise=0.0_num_iter=100_offset=200_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n",
      "2020-05-18 14:15:21,473 - Cache - INFO - Using cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=True_archi=mnist_lenet_e_0_attack_type=FGSM_art_compute_graph=False_dataset=mnist_dataset_size=200_epsilon=0.3_lims=(tensor(-0.5000), tensor(0.5000))_noise=0.0_num_iter=100_offset=200_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n",
      "2020-05-18 14:15:21,486 - Cache - INFO - Using cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/tda/../cache//get_sample_dataset/adv=True_archi=mnist_lenet_e_0_attack_type=FGSM_art_compute_graph=False_dataset=mnist_dataset_size=200_epsilon=0.4_lims=(tensor(-0.5000), tensor(0.5000))_noise=0.0_num_iter=100_offset=200_succ_adv=True_train=False_transfered_attacks=False.cached for the call to get_sample_dataset\n"
     ]
    }
   ],
   "source": [
    "# Build tda dataset (i.e activation graphs for clean and adversarial inputs)\n",
    "from tda.protocol import get_protocolar_datasets\n",
    "\n",
    "# %debug\n",
    "lims = X_train.min(), X_train.max()\n",
    "dataset_size = 200\n",
    "all_epsilons = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4]\n",
    "(train_clean, test_clean, train_adv,\n",
    " test_adv) = get_protocolar_datasets(dataset=dataset,\n",
    "                                     succ_adv=True,\n",
    "                                     dataset_size=dataset_size,\n",
    "                                     noise=0., compute_graph=False,\n",
    "                                     all_epsilons=all_epsilons,\n",
    "                                     attack_type=\"FGSM_art\",\n",
    "                                     archi=lenet_arch,\n",
    "                                     lims=lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tda.graph\n",
    "\n",
    "if False:\n",
    "    print(\"\\nComputing activation graphs for clean examples\")\n",
    "    for ds in train_clean, test_clean:\n",
    "        for l, line in enumerate(ds):\n",
    "            print(\".\", end=\"\")\n",
    "            if line.graph is None:\n",
    "                ds[l] = line = line._replace(\n",
    "                    graph=tda.graph.Graph.from_architecture_and_data_point(\n",
    "                        lenet_arch, line.x))\n",
    "        assert 0\n",
    "print(\"\\nComputing activation graphs for adversarial examples\", end=\"\")\n",
    "for ds in train_adv, test_adv:\n",
    "    for eps in ds:\n",
    "        if eps != 0.2: continue\n",
    "        print(\"\\neps=%.3f\" % eps)\n",
    "        for l, line in enumerate(ds[eps]):\n",
    "            print(\".\", end=\"\")\n",
    "            if line.graph is None:\n",
    "                ds[eps][l] = line._replace(\n",
    "                graph=tda.graph.Graph.from_architecture_and_data_point(\n",
    "                lenet_arch, line.x))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing test adversarial embeddings for eps=0.010\n",
      "\n",
      "Computing test adversarial embeddings for eps=0.050\n",
      "\n",
      "Computing test adversarial embeddings for eps=0.100\n"
     ]
    }
   ],
   "source": [
    "# Compute embeddings for test-set adversarial inputs\n",
    "from tda.embeddings import get_embedding, EmbeddingType, KernelType, ThresholdStrategy\n",
    "from joblib import delayed, Parallel\n",
    "\n",
    "\n",
    "def embedding_getter(line):\n",
    "    embedding = get_embedding(\n",
    "        architecture=lenet_arch,\n",
    "        embedding_type=EmbeddingType.PersistentDiagram,\n",
    "        line=line, dataset=None, edges_to_keep=None,\n",
    "        threshold_strategy=ThresholdStrategy.ActivationValue,\n",
    "        thresholds=thresholds)\n",
    "    print(\".\", end=\"\")\n",
    "    return embedding\n",
    "\n",
    "\n",
    "n_jobs = 2\n",
    "embeddings = {}\n",
    "for eps in test_adv:\n",
    "    print(\"\\nComputing test adversarial embeddings for eps=%.3f\" % eps)\n",
    "    embeddings[eps] = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(embedding_getter)(line)\n",
    "        for line in test_adv[eps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0] = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(embedding_getter)(line)\n",
    "        for line in test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_test_adv = dict((eps, embeddings[eps]) for eps in embeddings if eps != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute other adversarial examples\n",
    "embeddings_train_adv = {}\n",
    "for eps in train_adv:\n",
    "    print(\"\\nComputing train adversarial embeddings for eps=%.3f\" % eps)\n",
    "    embeddings_train_adv[eps] = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(embedding_getter)(line) for line in train_adv[eps])\n",
    "print(\"\\nComputing train clean embeddings\")\n",
    "embeddings_train_clean = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(embedding_getter)(line)\n",
    "        for line in train_clean)\n",
    "print(\"\\nComputing test clean embeddings\")\n",
    "embeddings_test_clean = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(embedding_getter)(line)\n",
    "        for line in test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The real deal: try to detect adversarial examples from normal examples\n",
    "from tda.protocol import evaluate_embeddings\n",
    "\n",
    "param_space = [{\"M\": 20, \"sigma\": sigma} for sigma in np.logspace(-3, 3, 7)]\n",
    "kernel_type = KernelType.SlicedWasserstein\n",
    "evaluation_results = evaluate_embeddings(embeddings_train_clean,\n",
    "                                         embeddings_test_clean,\n",
    "                                         embeddings_train_adv,\n",
    "                                         embeddings_test_adv,\n",
    "                                         kernel_type=kernel_type,\n",
    "                                         param_space=param_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize embeddings\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "cmap = cm.Blues_r\n",
    "\n",
    "colors = {0: \"b\",\n",
    "          0.1: \"c\",\n",
    "          0.2: \"m\",\n",
    "          0.3: \"r\"}\n",
    "_, (ax1, ax3, ax4) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for eps in embeddings:\n",
    "    if not eps in [0., 0.1, 0.3]: continue\n",
    "    color = colors[eps]\n",
    "    for x in embeddings[eps]:\n",
    "        birth, death = np.transpose(x)\n",
    "        age = death - birth\n",
    "        ax1.plot(birth, c=color)\n",
    "        ax1.set_ylabel(\"birth\")\n",
    "        ax1.set_xlabel(\"points\")\n",
    "        # ax2.plot(death, c=color)\n",
    "        # ax2.set_ylabel(\"death\")\n",
    "        # ax2.set_xlabel(\"points\")\n",
    "        ax3.plot(age, c=color)\n",
    "        ax3.set_ylabel(\"age (death - birth)\")\n",
    "        ax3.set_xlabel(\"points\")\n",
    "        ax4.scatter(birth, death, c=color);\n",
    "        ax4.set_xlabel(\"birth\")\n",
    "        ax4.set_ylabel(\"death\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance of the detector\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = []\n",
    "for key in [\"supervised_metrics\", \"unsupervised_metrics\"]:\n",
    "    tmp = evaluation_results[key]\n",
    "    if key == \"unsupervised_metrics\":\n",
    "        sup = False\n",
    "    else:\n",
    "        sup = True\n",
    "    for eps in tmp:\n",
    "        df.append(dict(sup=sup, eps=eps, auc=tmp[eps][\"auc\"][\"upper_bound\"],\n",
    "                       method=\"PersistentDiagram\",\n",
    "                       arch=lenet_arch.name))\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data=df, x=\"eps\", y=\"auc\", hue=\"sup\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_arch.layers[-2].get_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse.linalg as slinalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = graph.get_adjacency_matrix()\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(15, 8))\n",
    "data = lenet_arch.layers[0].get_matrix()[-1].todense()[:2000].T\n",
    "data = np.ma.masked_where(data == 0, data)\n",
    "ax.matshow(data, cmap=plt.cm.RdBu);\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph._edge_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_arch.layers[4].get_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tda.graph import Graph\n",
    "\n",
    "Graph.from_architecture_and_data_point(lenet_arch, X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph._edge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripser.ripser import Rips\n",
    "\n",
    "rips = Rips(maxdim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rips.fit_transform(graph.get_adjacency_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_list(graph):\n",
    "    \"\"\"\n",
    "    Generate the list of edges of the multipartite graph\n",
    "    \"\"\"\n",
    "    shapes = graph._get_shapes()\n",
    "    all_layer_indices = sorted(list(shapes.keys()))\n",
    "    vertex_offset = [0] + list(np.cumsum([shapes[idx]\n",
    "                                          for idx in all_layer_indices]))\n",
    "    vertex_offset = vertex_offset[:-1]\n",
    "    for source_layer, target_layer in graph._edge_dict:\n",
    "        offset_source = vertex_offset[source_layer + 1]\n",
    "        offset_target = vertex_offset[target_layer + 1]\n",
    "        mat = graph._edge_dict[(source_layer, target_layer)]\n",
    "        source_vertices = mat.col + offset_source\n",
    "        target_vertices = mat.row + offset_target\n",
    "        for edge, weight in zip(zip(source_vertices, target_vertices), mat.data):\n",
    "            yield edge, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit -n 1 graph.get_edge_list();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit -n 1 list(get_edge_list(graph));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.08 / .777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tda.embeddings import persistent_diagrams\n",
    "from tda import graph\n",
    "from imp import reload\n",
    "\n",
    "reload(persistent_diagrams)\n",
    "reload(graph)\n",
    "graph = graph.Graph.from_architecture_and_data_point(lenet_arch, X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toto = persistent_diagrams._prepare_edges_for_diagram_old(graph);\n",
    "titi = persistent_diagrams._prepare_edges_for_diagram_fast(graph);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titi = list(titi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titi[-1], toto[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vertices, w in persistent_diagrams._prepare_edges_for_diagram_fast(graph):\n",
    "    print(vertices, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_arch.layer_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tda.embeddings.raw_graph as raw_graph\n",
    "\n",
    "for layer_link in lenet_arch.layer_links[::-1]:\n",
    "    active_indices = raw_graph.identify_active_indices([raw_graph.to_sparse_vector(\n",
    "        line.graph._edge_dict[layer_link]) for line in train_clean])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line.graph._edge_dict[layer_link].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_clean[4].graph._edge_dict[layer_link].todense()\n",
    "b = train_clean[11].graph._edge_dict[layer_link].todense()\n",
    "c = train_adv[0.2][4].graph._edge_dict[layer_link].todense()\n",
    "plt.matshow(a[:, :200])\n",
    "plt.axis(\"off\")\n",
    "plt.matshow(b[:, :200])\n",
    "plt.axis(\"off\");\n",
    "plt.matshow(c[:, :200])\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "srp = SparseRandomProjection(random_state=0)\n",
    "\n",
    "proj_mat = srp._make_random_matrix(100, 11520 * 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(mean[:, :100])\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(proj_mat.dot(a.ravel().T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clean = np.array([line.y for line in train_clean])\n",
    "graph_clean = [line.graph for line in train_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.where(y_clean == 6)[0].build_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean([graph_clean[i]._edge_dict[layer_link].todense() for i in indices], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lenet_arch.layers[2].build_matrix(old=True).todense()\n",
    "b = lenet_arch.layers[2].build_matrix(old=False).todense()\n",
    "\n",
    "import numpy as np\n",
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit lenet_arch.layers[2].build_matrix(old=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit lenet_arch.layers[2].build_matrix(old=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building and initializing cifar100 parameters\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (9): ReLU()\n",
      "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (12): ReLU()\n",
      "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (14): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (16): ReLU()\n",
      "  (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (19): ReLU()\n",
      "  (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (21): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (22): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (23): ReLU()\n",
      "  (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=100, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building CIFAR-100 data loader with 1 workers\n",
      "Files already downloaded and verified\n",
      "Building CIFAR-100 data loader with 1 workers\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../universal-attacks/\")\n",
    "\n",
    "from imp import reload\n",
    "import torch\n",
    "import universal_attacks.models as models\n",
    "reload(models)\n",
    "\n",
    "# load pretrained model and prepare dataset fetcher\n",
    "model, ds_fetcher, class_names = models.get_cifar100(\n",
    "    model_root=\"pytorch_data\")\n",
    "class_indices = dict(zip(class_names.values(), class_names.keys()))\n",
    "\n",
    "# turn off dropout\n",
    "model.eval()\n",
    "\n",
    "# fetch data\n",
    "for X_train, y_train in ds_fetcher(batch_size=1000, train=True, val=False):\n",
    "    break\n",
    "for X_test, y_test in ds_fetcher(batch_size=50000, train=False, val=True):\n",
    "    break\n",
    "    \n",
    "# save mem\n",
    "if False:\n",
    "  model = model.float()\n",
    "  X_train = X_train.float()\n",
    "  y_train = y_train.float()\n",
    "  X_test = X_test.float()\n",
    "  y_test = y_test.float()\n",
    "\n",
    "# transfer data to GPU\n",
    "if torch.cuda.is_available():\n",
    "    X_train = X_train.cuda()\n",
    "    y_train = y_train.cuda()\n",
    "    X_test = X_test.cuda()\n",
    "    y_test = y_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-17 08:55:18,670 - ConvLayer - INFO - <tda.models.layers.conv_layer.ConvLayer object at 0x7f778f53ecd0> received input with shape torch.Size([1, 3, 32, 32])\n",
      "<tda.models.layers.conv_layer.ConvLayer object at 0x7f778f53ecd0> received input with shape torch.Size([1, 3, 32, 32])\n",
      "2020-05-17 08:55:18,673 - ConvLayer - INFO - <tda.models.layers.conv_layer.ConvLayer object at 0x7f778e8e9cd0> received input with shape torch.Size([1, 128, 32, 32])\n",
      "<tda.models.layers.conv_layer.ConvLayer object at 0x7f778e8e9cd0> received input with shape torch.Size([1, 128, 32, 32])\n",
      "2020-05-17 08:55:18,688 - ConvLayer - INFO - <tda.models.layers.conv_layer.ConvLayer object at 0x7f778e8e92d0> received input with shape torch.Size([1, 128, 16, 16])\n",
      "<tda.models.layers.conv_layer.ConvLayer object at 0x7f778e8e92d0> received input with shape torch.Size([1, 128, 16, 16])\n",
      "2020-05-17 08:55:18,694 - ConvLayer - INFO - <tda.models.layers.conv_layer.ConvLayer object at 0x7f778e8e95d0> received input with shape torch.Size([1, 256, 16, 16])\n",
      "<tda.models.layers.conv_layer.ConvLayer object at 0x7f778e8e95d0> received input with shape torch.Size([1, 256, 16, 16])\n",
      "2020-05-17 08:55:18,707 - ConvLayer - INFO - <tda.models.layers.conv_layer.ConvLayer object at 0x7f778e8e9710> received input with shape torch.Size([1, 256, 8, 8])\n",
      "<tda.models.layers.conv_layer.ConvLayer object at 0x7f778e8e9710> received input with shape torch.Size([1, 256, 8, 8])\n",
      "2020-05-17 08:55:18,714 - ConvLayer - INFO - <tda.models.layers.conv_layer.ConvLayer object at 0x7f778e8e9f50> received input with shape torch.Size([1, 512, 8, 8])\n",
      "<tda.models.layers.conv_layer.ConvLayer object at 0x7f778e8e9f50> received input with shape torch.Size([1, 512, 8, 8])\n",
      "2020-05-17 08:55:18,728 - ConvLayer - INFO - <tda.models.layers.conv_layer.ConvLayer object at 0x7f778e94c510> received input with shape torch.Size([1, 512, 4, 4])\n",
      "<tda.models.layers.conv_layer.ConvLayer object at 0x7f778e94c510> received input with shape torch.Size([1, 512, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "reload(parser)\n",
    "arch = parser.model_to_architecture(model, x=X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tda.graph import Graph\n",
    "\n",
    "graph = Graph.from_architecture_and_data_point(arch, X_train[0])\n",
    "for key in graph._edge_dict:\n",
    "    layer_matrix = graph._edge_dict[key]\n",
    "    print(layer_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in lenet.named_children():\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unroll(model):\n",
    "    for child in model.children():\n",
    "        if len(list(child.children())):\n",
    "            for grandchild in _unroll(child):\n",
    "                yield grandchild\n",
    "        else:\n",
    "            yield child\n",
    "\n",
    "for c in _unroll(model.features):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lprint(a):\n",
    "    if isinstance(a, list):\n",
    "        for i in a:\n",
    "            yield from lprint(i)\n",
    "    else:\n",
    "        yield a\n",
    "\n",
    "a = [[1, [2, 3], 4], [5, 6, [7, 8, [9]]]]\n",
    "for i in lprint(b):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
