{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-04 17:22:02,475 - Devices - INFO - Found 0 devices compatible with CUDA\n"
     ]
    }
   ],
   "source": [
    "from tda.experiments.ocsvm_detector.ocsvm_detector_binary import get_all_embeddings, Config\n",
    "from tda.embeddings import EmbeddingType, KernelType\n",
    "from tda.models.architectures import mnist_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration of the experiment\n",
    "epsilons = [.02, .04, .06,  .08, .1, .12][:1]\n",
    "n_jobs = 1\n",
    "config = Config(\n",
    "    embedding_type=EmbeddingType.RawGraph,\n",
    "    kernel_type=KernelType.SlicedWasserstein,\n",
    "    thresholds='0.1',\n",
    "    epochs=25,\n",
    "    dataset=\"MNIST\",\n",
    "    architecture=mnist_mlp.name,\n",
    "    train_noise=0.0,\n",
    "    dataset_size=200,\n",
    "    successful_adv=1,\n",
    "    attack_type=\"FGSM\",\n",
    "    noise=0.0,\n",
    "    \n",
    "    n_jobs=n_jobs,\n",
    "    all_epsilons = epsilons,\n",
    "\n",
    "    num_iter=1,\n",
    "    height=1,\n",
    "    hash_size=1,\n",
    "    node_labels=0,\n",
    "    steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-04 17:22:13,277 - Datasets - INFO - Instantiated dataset MNIST with validation_size 1000\n",
      "2020-02-04 17:22:13,278 - Models - INFO - Filename = /home/elvis/CODE/FORKED/TDA_for_adv_robustness/trained_models/mnist_simple_fcn_mnist_25_epochs.model \n",
      "\n",
      "/home/elvis/anaconda3/envs/tda/lib/python3.6/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'tda.models.architectures.Architecture' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "2020-02-04 17:22:13,288 - Models - INFO - Loaded successfully model from /home/elvis/CODE/FORKED/TDA_for_adv_robustness/trained_models/mnist_simple_fcn_mnist_25_epochs.model\n",
      "2020-02-04 17:22:13,289 - Thresholds - INFO - Detected uniform threshold\n",
      "2020-02-04 17:22:13,289 - Thresholds - INFO - My received thresholds {(-1, 0): 0.1, (0, 1): 0.1, (1, 2): 0.1}\n",
      "2020-02-04 17:22:13,290 - GraphStats - INFO - Loading stats from file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/stats/architecture=simple_fcn_mnist/dataset=mnist/dataset_size=100/train_noise=0.0/stats.pickle\n",
      "2020-02-04 17:22:13,293 - Thresholds - INFO - Link (-1, 0): threshold=1476.3588066664477 (quantile 0.1)\n",
      "2020-02-04 17:22:13,294 - Thresholds - INFO - Link (0, 1): threshold=569.3744718494883 (quantile 0.1)\n",
      "2020-02-04 17:22:13,294 - Thresholds - INFO - Link (1, 2): threshold=9175.272671444942 (quantile 0.1)\n",
      "2020-02-04 17:22:13,294 - Thresholds - INFO - Thresholds = {(-1, 0): 1476.3588066664477, (0, 1): 569.3744718494883, (1, 2): 9175.272671444942}\n",
      "2020-02-04 17:22:13,295 - C3PO - INFO - I will produce for you the protocolar datasets !\n",
      "2020-02-04 17:22:13,296 - GraphDataset - INFO - Using source dataset mnist\n",
      "2020-02-04 17:22:13,296 - GraphDataset - INFO - Checking that the received architecture has been trained\n",
      "2020-02-04 17:22:13,297 - GraphDataset - INFO - OK ! Architecture is ready\n",
      "2020-02-04 17:22:13,297 - GraphDataset - INFO - I am going to generate a dataset of 100 points...\n",
      "2020-02-04 17:22:13,297 - GraphDataset - INFO - Only successful adversaries ? yes\n",
      "2020-02-04 17:22:13,297 - GraphDataset - INFO - Which attack ? FGSM\n",
      "2020-02-04 17:22:13,314 - GraphDataset - INFO - computing sample number = 10/100\n",
      "2020-02-04 17:22:13,321 - GraphDataset - INFO - computing sample number = 20/100\n",
      "2020-02-04 17:22:13,327 - GraphDataset - INFO - computing sample number = 30/100\n",
      "2020-02-04 17:22:13,332 - GraphDataset - INFO - computing sample number = 40/100\n",
      "2020-02-04 17:22:13,339 - GraphDataset - INFO - computing sample number = 50/100\n",
      "2020-02-04 17:22:13,347 - GraphDataset - INFO - computing sample number = 60/100\n",
      "2020-02-04 17:22:13,353 - GraphDataset - INFO - computing sample number = 70/100\n",
      "2020-02-04 17:22:13,358 - GraphDataset - INFO - computing sample number = 80/100\n",
      "2020-02-04 17:22:13,364 - GraphDataset - INFO - computing sample number = 90/100\n",
      "2020-02-04 17:22:13,369 - GraphDataset - INFO - computing sample number = 100/100\n",
      "2020-02-04 17:22:13,370 - Cache - INFO - Creating cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/cache/get_sample_dataset/adv=False_archi=simple_fcn_mnist_dataset=mnist_dataset_size=100_epsilon=0.0_noise=0.0_offset=0_succ_adv=True_train=False.cached for the call to get_sample_dataset\n",
      "2020-02-04 17:22:13,375 - GraphDataset - INFO - Using source dataset mnist\n",
      "2020-02-04 17:22:13,375 - GraphDataset - INFO - Checking that the received architecture has been trained\n",
      "2020-02-04 17:22:13,376 - GraphDataset - INFO - OK ! Architecture is ready\n",
      "2020-02-04 17:22:13,376 - GraphDataset - INFO - I am going to generate a dataset of 100 points...\n",
      "2020-02-04 17:22:13,377 - GraphDataset - INFO - Only successful adversaries ? yes\n",
      "2020-02-04 17:22:13,377 - GraphDataset - INFO - Which attack ? FGSM\n",
      "2020-02-04 17:22:13,386 - GraphDataset - INFO - computing sample number = 10/100\n",
      "2020-02-04 17:22:13,392 - GraphDataset - INFO - computing sample number = 20/100\n",
      "2020-02-04 17:22:13,399 - GraphDataset - INFO - computing sample number = 30/100\n",
      "2020-02-04 17:22:13,405 - GraphDataset - INFO - computing sample number = 40/100\n",
      "2020-02-04 17:22:13,411 - GraphDataset - INFO - computing sample number = 50/100\n",
      "2020-02-04 17:22:13,418 - GraphDataset - INFO - computing sample number = 60/100\n",
      "2020-02-04 17:22:13,424 - GraphDataset - INFO - computing sample number = 70/100\n",
      "2020-02-04 17:22:13,431 - GraphDataset - INFO - computing sample number = 80/100\n",
      "2020-02-04 17:22:13,438 - GraphDataset - INFO - computing sample number = 90/100\n",
      "2020-02-04 17:22:13,447 - GraphDataset - INFO - computing sample number = 100/100\n",
      "2020-02-04 17:22:13,447 - Cache - INFO - Creating cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/cache/get_sample_dataset/adv=False_archi=simple_fcn_mnist_dataset=mnist_dataset_size=100_epsilon=0.0_noise=0.0_offset=100_succ_adv=True_train=False.cached for the call to get_sample_dataset\n",
      "2020-02-04 17:22:13,454 - GraphDataset - INFO - Using source dataset mnist\n",
      "2020-02-04 17:22:13,455 - GraphDataset - INFO - Checking that the received architecture has been trained\n",
      "2020-02-04 17:22:13,455 - GraphDataset - INFO - OK ! Architecture is ready\n",
      "2020-02-04 17:22:13,455 - GraphDataset - INFO - I am going to generate a dataset of 200 points...\n",
      "2020-02-04 17:22:13,456 - GraphDataset - INFO - Only successful adversaries ? yes\n",
      "2020-02-04 17:22:13,456 - GraphDataset - INFO - Which attack ? FGSM\n",
      "2020-02-04 17:22:13,700 - GraphDataset - INFO - computing sample number = 10/200\n",
      "2020-02-04 17:22:13,983 - GraphDataset - INFO - computing sample number = 20/200\n",
      "2020-02-04 17:22:14,186 - GraphDataset - INFO - computing sample number = 30/200\n",
      "2020-02-04 17:22:14,386 - GraphDataset - INFO - computing sample number = 40/200\n",
      "2020-02-04 17:22:14,916 - GraphDataset - INFO - computing sample number = 50/200\n",
      "2020-02-04 17:22:15,116 - GraphDataset - INFO - computing sample number = 60/200\n",
      "2020-02-04 17:22:15,471 - GraphDataset - INFO - computing sample number = 70/200\n",
      "2020-02-04 17:22:15,730 - GraphDataset - INFO - computing sample number = 80/200\n",
      "2020-02-04 17:22:16,109 - GraphDataset - INFO - computing sample number = 90/200\n",
      "2020-02-04 17:22:16,356 - GraphDataset - INFO - computing sample number = 100/200\n",
      "2020-02-04 17:22:16,814 - GraphDataset - INFO - computing sample number = 110/200\n",
      "2020-02-04 17:22:17,105 - GraphDataset - INFO - computing sample number = 120/200\n",
      "2020-02-04 17:22:17,408 - GraphDataset - INFO - computing sample number = 130/200\n",
      "2020-02-04 17:22:17,573 - GraphDataset - INFO - computing sample number = 140/200\n",
      "2020-02-04 17:22:17,842 - GraphDataset - INFO - computing sample number = 150/200\n",
      "2020-02-04 17:22:18,165 - GraphDataset - INFO - computing sample number = 160/200\n",
      "2020-02-04 17:22:18,304 - GraphDataset - INFO - computing sample number = 170/200\n",
      "2020-02-04 17:22:18,597 - GraphDataset - INFO - computing sample number = 180/200\n",
      "2020-02-04 17:22:19,056 - GraphDataset - INFO - computing sample number = 190/200\n",
      "2020-02-04 17:22:19,282 - GraphDataset - INFO - computing sample number = 200/200\n",
      "2020-02-04 17:22:19,283 - Cache - INFO - Creating cache file /home/elvis/CODE/FORKED/TDA_for_adv_robustness/cache/get_sample_dataset/adv=True_archi=simple_fcn_mnist_attack_type=FGSM_dataset=mnist_dataset_size=200_epsilon=0.02_noise=0.0_num_iter=50_offset=200_succ_adv=True_train=False.cached for the call to get_sample_dataset\n",
      "2020-02-04 17:22:21,599 - Detector - INFO - Clean train dataset (100 points) !!\n",
      "2020-02-04 17:22:24,058 - Detector - INFO - Clean test dataset (100 points) !!\n",
      "2020-02-04 17:22:26,378 - Detector - INFO - Adversarial train dataset for espilon = 0.02  (100 points) !\n",
      "2020-02-04 17:22:28,626 - Detector - INFO - Adversarial test dataset for espilon = 0.02 (100 points)  !\n"
     ]
    }
   ],
   "source": [
    "# compute activation graphs\n",
    "(clean_graphs_train, clean_graphs_test, adv_graphs_train, adv_graphs_test, thresholds,\n",
    " stats, stats_inf) = get_all_embeddings(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from sklearn.base import BaseEstimator\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "class GraphMasker(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Featurizer for activation graphs\n",
    "    \"\"\"\n",
    "    def __init__(self, n_jobs=1):\n",
    "        super(GraphMasker, self).__init__()\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._compute_mask(X)\n",
    "        return self\n",
    "        \n",
    "    def _compute_mask(self, X):\n",
    "        \"\"\"\n",
    "        Determine edges which are alive in all samples\n",
    "        \"\"\"\n",
    "        print(\"\\tComputing common mask\")\n",
    "        support = set(zip(*X[0].get_adjacency_matrix().nonzero()))\n",
    "        for graph in X[1:]:\n",
    "            i, j = graph.get_adjacency_matrix().nonzero()\n",
    "            support = support.intersection(zip(i, j))\n",
    "        self.support_i, self.support_j = zip(*support)\n",
    "        self.nonzero_count = len(self.support_i)\n",
    "\n",
    "    def _apply_mask(self, graph) -> typing.List[float]:\n",
    "        \"\"\"\n",
    "        Featurize a graph\n",
    "        \"\"\"\n",
    "        return graph.get_adjacency_matrix().toarray()[self.support_i, self.support_j] / -10e5\n",
    "    \n",
    "    def transform(self, X) -> typing.List[typing.List[float]]:\n",
    "        return Parallel(n_jobs=self.n_jobs)(delayed(self._apply_mask)(x) for x in X)\n",
    "    \n",
    "    def fit_transform(self, X) -> typing.List[typing.List[float]]:\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizing clean training data\n",
      "\tComputing common mask\n",
      "Featurizing clean test data\n",
      "Featuring adv test data\n"
     ]
    }
   ],
   "source": [
    "# featurize all activation graphs (may take a while)\n",
    "masker = GraphMasker(n_jobs=config.n_jobs)\n",
    "print(\"Featurizing clean training data\")\n",
    "clean_X_train = masker.fit_transform(clean_graphs_train)\n",
    "print(\"Featurizing clean test data\")\n",
    "clean_X_test = masker.transform(clean_graphs_test)\n",
    "# adv_X_train = dict((eps, masker.transform(graphs))\n",
    "#                    for eps, graphs in adv_graphs_train.items())\n",
    "print(\"Featuring adv test data\")\n",
    "adv_X_test = dict((eps, masker.transform(graphs))\n",
    "                   for eps, graphs in adv_graphs_test.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build scikit-learn pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_components = 20\n",
    "random_state = 0\n",
    "pca = PCA(n_components=n_components, random_state=random_state)\n",
    "detector = Pipeline([(\"scaler\", StandardScaler()),  # get your scaling right!\n",
    "                     (\"pca\", pca),  # fix curse of dimensionality\n",
    "                     (\"ocsvm\", OneClassSVM())  # the actual detector\n",
    "                    ])\n",
    "detector.fit(clean_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from colour import Color\n",
    "blue = Color(\"blue\")\n",
    "colors = list(blue.range_to(Color(\"red\"), 1 + len(epsilons)))\n",
    "colors = [np.array(color.get_rgb())[None, :] for color in colors]\n",
    "codes = dict((eps, pca.transform(adv_X_test[eps]))\n",
    "              for eps in adv_X_test)\n",
    "codes[0] = pca.transform(clean_X_test)\n",
    "for c, eps in zip(colors, np.append(0, epsilons)):\n",
    "    plt.scatter(*codes[eps].T[:2], c=c, marker=\"o\")\n",
    "# plt.ylim(-.01, .01)\n",
    "# plt.xlim(-.05, .05)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute scores\n",
    "clean_labels = np.ones(len(clean_X_test))\n",
    "clean_labels_pred = detector.score_samples(clean_X_test)\n",
    "scores = []\n",
    "adv_X_test[0] = clean_X_test\n",
    "for eps in np.append(0, epsilons):\n",
    "    adv_labels_pred = detector.score_samples(adv_X_test[eps])\n",
    "    adv_labels = np.zeros(len(adv_X_test[eps]))\n",
    "    labels = np.append(clean_labels, adv_labels)\n",
    "    labels_pred = np.append(clean_labels_pred, adv_labels_pred)\n",
    "    score = roc_auc_score(labels, labels_pred)\n",
    "    print(\"eps = %0.2f, AUC = %0.2f\" % (eps, score))\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "plt.plot(np.append(0, epsilons), scores, linewidth=2)\n",
    "plt.xlabel(\"$\\\\varepsilon$\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
