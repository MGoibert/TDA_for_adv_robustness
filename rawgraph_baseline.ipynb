{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-03 22:20:40,072 - Devices - INFO - Found 0 devices compatible with CUDA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find dionysus\n"
     ]
    }
   ],
   "source": [
    "from tda.experiments.ocsvm_detector.ocsvm_detector_binary import get_all_embeddings, Config\n",
    "from tda.embeddings import EmbeddingType, KernelType\n",
    "from tda.models.architectures import mnist_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration of the experiment\n",
    "epsilons = [.02, .04, .06,  .08, .1, .12]\n",
    "n_jobs = 1\n",
    "config = Config(\n",
    "    embedding_type=EmbeddingType.RawGraph,\n",
    "    kernel_type=KernelType.SlicedWasserstein,\n",
    "    thresholds='0.1',\n",
    "    epochs=25,\n",
    "    dataset=\"MNIST\",\n",
    "    architecture=mnist_mlp.name,\n",
    "    train_noise=0.0,\n",
    "    dataset_size=200,\n",
    "    successful_adv=1,\n",
    "    attack_type=\"FGSM\",\n",
    "    noise=0.0,\n",
    "    \n",
    "    n_jobs=n_jobs,\n",
    "    all_epsilons = epsilons,\n",
    "\n",
    "    num_iter=1,\n",
    "    height=1,\n",
    "    hash_size=1,\n",
    "    node_labels=0,\n",
    "    steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-03 22:20:56,779 - Datasets - INFO - Instantiated dataset MNIST with validation_size 1000\n",
      "2020-02-03 22:20:56,780 - Models - INFO - Filename = C:\\Users\\e.dohmatob\\Downloads\\tda/trained_models/mnist_simple_fcn_mnist_25_epochs.model \n",
      "\n",
      "C:\\Users\\e.dohmatob\\AppData\\Local\\Continuum\\anaconda3\\envs\\tda\\lib\\site-packages\\torch\\serialization.py:593: SourceChangeWarning: source code of class 'tda.models.architectures.Architecture' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "2020-02-03 22:20:56,811 - Models - INFO - Loaded successfully model from C:\\Users\\e.dohmatob\\Downloads\\tda/trained_models/mnist_simple_fcn_mnist_25_epochs.model\n",
      "2020-02-03 22:20:56,814 - Thresholds - INFO - Detected uniform threshold\n",
      "2020-02-03 22:20:56,817 - Thresholds - INFO - My received thresholds {(-1, 0): 0.1, (0, 1): 0.1, (1, 2): 0.1}\n",
      "2020-02-03 22:20:56,822 - GraphStats - INFO - Loading stats from file C:\\Users\\e.dohmatob\\Downloads\\tda/stats/architecture=simple_fcn_mnist/dataset=mnist/dataset_size=5/train_noise=0.0/stats.pickle\n",
      "2020-02-03 22:20:56,840 - Thresholds - INFO - Link (-1, 0): threshold=1498.5464787270564 (quantile 0.1)\n",
      "2020-02-03 22:20:56,842 - Thresholds - INFO - Link (0, 1): threshold=604.5938491921717 (quantile 0.1)\n",
      "2020-02-03 22:20:56,843 - Thresholds - INFO - Link (1, 2): threshold=8637.985777218171 (quantile 0.1)\n",
      "2020-02-03 22:20:56,846 - Thresholds - INFO - Thresholds = {(-1, 0): 1498.5464787270564, (0, 1): 604.5938491921717, (1, 2): 8637.985777218171}\n",
      "2020-02-03 22:20:56,851 - C3PO - INFO - I will produce for you the protocolar datasets !\n",
      "2020-02-03 22:20:56,853 - Cache - INFO - Using cache file C:\\Users\\e.dohmatob\\Downloads\\tda/cache/get_sample_dataset/adv=False_archi=simple_fcn_mnist_dataset=mnist_dataset_size=100_epsilon=0.0_noise=0.0_offset=0_succ_adv=True_train=False.cached for the call to get_sample_dataset\n",
      "2020-02-03 22:20:56,868 - Cache - INFO - Using cache file C:\\Users\\e.dohmatob\\Downloads\\tda/cache/get_sample_dataset/adv=False_archi=simple_fcn_mnist_dataset=mnist_dataset_size=100_epsilon=0.0_noise=0.0_offset=100_succ_adv=True_train=False.cached for the call to get_sample_dataset\n",
      "2020-02-03 22:20:56,882 - Cache - INFO - Using cache file C:\\Users\\e.dohmatob\\Downloads\\tda/cache/get_sample_dataset/adv=True_archi=simple_fcn_mnist_attack_type=FGSM_dataset=mnist_dataset_size=200_epsilon=0.02_noise=0.0_num_iter=50_offset=200_succ_adv=True_train=False.cached for the call to get_sample_dataset\n",
      "2020-02-03 22:20:56,904 - Cache - INFO - Using cache file C:\\Users\\e.dohmatob\\Downloads\\tda/cache/get_sample_dataset/adv=True_archi=simple_fcn_mnist_attack_type=FGSM_dataset=mnist_dataset_size=200_epsilon=0.04_noise=0.0_num_iter=50_offset=200_succ_adv=True_train=False.cached for the call to get_sample_dataset\n",
      "2020-02-03 22:20:56,936 - Cache - INFO - Using cache file C:\\Users\\e.dohmatob\\Downloads\\tda/cache/get_sample_dataset/adv=True_archi=simple_fcn_mnist_attack_type=FGSM_dataset=mnist_dataset_size=200_epsilon=0.06_noise=0.0_num_iter=50_offset=200_succ_adv=True_train=False.cached for the call to get_sample_dataset\n",
      "2020-02-03 22:20:56,964 - Cache - INFO - Using cache file C:\\Users\\e.dohmatob\\Downloads\\tda/cache/get_sample_dataset/adv=True_archi=simple_fcn_mnist_attack_type=FGSM_dataset=mnist_dataset_size=200_epsilon=0.08_noise=0.0_num_iter=50_offset=200_succ_adv=True_train=False.cached for the call to get_sample_dataset\n",
      "2020-02-03 22:20:56,985 - Cache - INFO - Using cache file C:\\Users\\e.dohmatob\\Downloads\\tda/cache/get_sample_dataset/adv=True_archi=simple_fcn_mnist_attack_type=FGSM_dataset=mnist_dataset_size=200_epsilon=0.1_noise=0.0_num_iter=50_offset=200_succ_adv=True_train=False.cached for the call to get_sample_dataset\n",
      "2020-02-03 22:20:57,017 - Cache - INFO - Using cache file C:\\Users\\e.dohmatob\\Downloads\\tda/cache/get_sample_dataset/adv=True_archi=simple_fcn_mnist_attack_type=FGSM_dataset=mnist_dataset_size=200_epsilon=0.12_noise=0.0_num_iter=50_offset=200_succ_adv=True_train=False.cached for the call to get_sample_dataset\n",
      "2020-02-03 22:20:57,070 - Detector - INFO - Clean train dataset !!\n",
      "2020-02-03 22:21:03,368 - Detector - INFO - Clean test dataset !!\n",
      "2020-02-03 22:21:09,344 - Detector - INFO - Adversarial train dataset for espilon = 0.02 !!\n",
      "2020-02-03 22:21:15,461 - Detector - INFO - Adversarial test dataset for espilon = 0.02 !!\n",
      "2020-02-03 22:21:20,970 - Detector - INFO - Adversarial train dataset for espilon = 0.04 !!\n",
      "2020-02-03 22:21:26,475 - Detector - INFO - Adversarial test dataset for espilon = 0.04 !!\n",
      "2020-02-03 22:21:31,903 - Detector - INFO - Adversarial train dataset for espilon = 0.06 !!\n",
      "2020-02-03 22:21:37,337 - Detector - INFO - Adversarial test dataset for espilon = 0.06 !!\n",
      "2020-02-03 22:21:42,909 - Detector - INFO - Adversarial train dataset for espilon = 0.08 !!\n",
      "2020-02-03 22:21:48,457 - Detector - INFO - Adversarial test dataset for espilon = 0.08 !!\n",
      "2020-02-03 22:21:54,350 - Detector - INFO - Adversarial train dataset for espilon = 0.1 !!\n",
      "2020-02-03 22:22:00,159 - Detector - INFO - Adversarial test dataset for espilon = 0.1 !!\n",
      "2020-02-03 22:22:05,839 - Detector - INFO - Adversarial train dataset for espilon = 0.12 !!\n",
      "2020-02-03 22:22:11,450 - Detector - INFO - Adversarial test dataset for espilon = 0.12 !!\n"
     ]
    }
   ],
   "source": [
    "# compute activation graphs\n",
    "(clean_graphs_train, clean_graphs_test, adv_graphs_train, adv_graphs_test, thresholds,\n",
    " stats, stats_inf) = get_all_embeddings(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from sklearn.base import BaseEstimator\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "class GraphMasker(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Featurizer for activation graphs\n",
    "    \"\"\"\n",
    "    def __init__(self, n_jobs=1):\n",
    "        super(GraphMasker, self).__init__()\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._compute_mask(X)\n",
    "        return self\n",
    "        \n",
    "    def _compute_mask(self, X):\n",
    "        \"\"\"\n",
    "        Determine edges which are alive in all samples\n",
    "        \"\"\"\n",
    "        print(\"\\tComputing common mask\")\n",
    "        support = set(zip(*X[0].get_adjacency_matrix().nonzero()))\n",
    "        for graph in X[1:]:\n",
    "            i, j = graph.get_adjacency_matrix().nonzero()\n",
    "            support = support.intersection(zip(i, j))\n",
    "        self.support_i, self.support_j = zip(*support)\n",
    "        self.nonzero_count = len(self.support_i)\n",
    "\n",
    "    def _apply_mask(self, graph) -> typing.List[float]:\n",
    "        \"\"\"\n",
    "        Featurize a graph\n",
    "        \"\"\"\n",
    "        return graph.get_adjacency_matrix().toarray()[self.support_i, self.support_j] / -10e5\n",
    "    \n",
    "    def transform(self, X) -> typing.List[typing.List[float]]:\n",
    "        return Parallel(n_jobs=self.n_jobs)(delayed(self._apply_mask)(x) for x in X)\n",
    "    \n",
    "    def fit_transform(self, X) -> typing.List[typing.List[float]]:\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizing clean training data\n",
      "\tComputing common mask\n",
      "Featurizing clean test data\n",
      "Featuring adv test data\n"
     ]
    }
   ],
   "source": [
    "# featurize all activation graphs (may take a while)\n",
    "masker = GraphMasker(n_jobs=config.n_jobs)\n",
    "print(\"Featurizing clean training data\")\n",
    "clean_X_train = masker.fit_transform(clean_graphs_train)\n",
    "print(\"Featurizing clean test data\")\n",
    "clean_X_test = masker.transform(clean_graphs_test)\n",
    "# adv_X_train = dict((eps, masker.transform(graphs))\n",
    "#                    for eps, graphs in adv_graphs_train.items())\n",
    "print(\"Featuring adv test data\")\n",
    "adv_X_test = dict((eps, masker.transform(graphs))\n",
    "                   for eps, graphs in adv_graphs_test.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build scikit-learn pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_components = 20\n",
    "random_state = 0\n",
    "pca = PCA(n_components=n_components, random_state=random_state)\n",
    "detector = Pipeline([(\"scaler\", StandardScaler()),  # get your scaling right!\n",
    "                     (\"pca\", pca),  # fix curse of dimensionality\n",
    "                     (\"ocsvm\", OneClassSVM())  # the actual detector\n",
    "                    ])\n",
    "detector.fit(clean_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from colour import Color\n",
    "blue = Color(\"blue\")\n",
    "colors = list(blue.range_to(Color(\"red\"), 1 + len(epsilons)))\n",
    "colors = [np.array(color.get_rgb())[None, :] for color in colors]\n",
    "codes = dict((eps, pca.transform(adv_X_test[eps]))\n",
    "              for eps in adv_X_test)\n",
    "codes[0] = pca.transform(clean_X_test)\n",
    "for c, eps in zip(colors, np.append(0, epsilons)):\n",
    "    plt.scatter(*codes[eps].T[:2], c=c, marker=\"o\")\n",
    "# plt.ylim(-.01, .01)\n",
    "# plt.xlim(-.05, .05)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute scores\n",
    "clean_labels = np.ones(len(clean_X_test))\n",
    "clean_labels_pred = detector.score_samples(clean_X_test)\n",
    "scores = []\n",
    "adv_X_test[0] = clean_X_test\n",
    "for eps in np.append(0, epsilons):\n",
    "    adv_labels_pred = detector.score_samples(adv_X_test[eps])\n",
    "    adv_labels = np.zeros(len(adv_X_test[eps]))\n",
    "    labels = np.append(clean_labels, adv_labels)\n",
    "    labels_pred = np.append(clean_labels_pred, adv_labels_pred)\n",
    "    score = roc_auc_score(labels, labels_pred)\n",
    "    print(\"eps = %0.2f, AUC = %0.2f\" % (eps, score))\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "plt.plot(np.append(0, epsilons), scores, linewidth=2)\n",
    "plt.xlabel(\"$\\\\varepsilon$\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
